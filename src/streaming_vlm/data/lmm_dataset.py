from dataclasses import dataclass
import json, torch, tqdm, io, os
from PIL import Image
from torch.utils.data import Dataset
from transformers import logging, AutoProcessor, Qwen2_5_VLForConditionalGeneration
from torchvision.transforms.functional import pil_to_tensor
from collections import defaultdict
from livecc_utils import _read_video_decord_plus, _spatial_resize_video
from typing import List
from qwen_vl_utils.vision_process import smart_nframes, process_vision_info, FPS
from ..utils.get_qwen_range import get_qwen_range
from transformers import set_seed


class mute_stderr_ffmpeg:
    def __enter__(self):
        self._stderr_fd = os.dup(2)
        self._devnull = os.open(os.devnull, os.O_WRONLY)
        os.dup2(self._devnull, 2)
        return self

    def __exit__(self, *exc):
        os.dup2(self._stderr_fd, 2)
        os.close(self._devnull)
        os.close(self._stderr_fd)


logger = logging.get_logger(__name__)


@dataclass
class DataArguments:
    train_annotation_paths: list[str] = None
    initial_fps_frames: int = int(FPS)
    streaming_fps_frames: int = int(FPS)
    with_context: bool = False
    text_sink: int = 0
    text_sliding_window: int = 0


@dataclass
class EvalDataArguments:
    eval_annotation_paths: list[str] = None


def readlastline(path: str):
    """Efficiently read the last line of a file."""
    with open(path, "rb") as f:
        f.seek(-2, 2)
        while f.read(1) != b"\n":
            f.seek(-2, 1)
        return f.readline()


def bytes_to_pil(image_bytes):
    """Convert bytes to a PIL Image object."""
    image = Image.open(io.BytesIO(image_bytes))
    if image.mode == "P":
        image = image.convert("RGBA")
    return image.convert("RGB")


def get_phrase_before_timestamp(text_stream, timestamp, start_from: int = 0):
    """From a word stream with timestamps, extract the phrase consisting of all words whose end time is before the given timestamp."""
    phrase = ""
    i = 0
    for i, (ws, we, word) in enumerate(text_stream[start_from:]):
        if timestamp >= we:
            phrase += " " + word.strip()
            if i == len(text_stream[start_from:]) - 1:
                i += 1
                break
        else:
            break
    return phrase, i + start_from


class LMMDataset(Dataset):
    """PyTorch dataset for multimodal large language models."""

    def __init__(
        self,
        *,
        train_annotation_paths: list[str] = None,
        eval_annotation_paths: list[str] = None,
        processor: AutoProcessor,
        tokenizer=None,
        initial_fps_frames: int = DataArguments.initial_fps_frames,
        streaming_fps_frames: int = DataArguments.streaming_fps_frames,
        with_context: str = DataArguments.with_context,
        return_conversation: bool = False,
        text_sink: int = DataArguments.text_sink,
        text_sliding_window: int = DataArguments.text_sliding_window,
        **kwargs,
    ):
        super().__init__()
        self.return_conversation = return_conversation
        self.handles = []
        if eval_annotation_paths is not None:
            self.is_eval_dataset = True
            annotation_paths = eval_annotation_paths
        else:
            self.is_eval_dataset = False
            annotation_paths = train_annotation_paths

        for annotation_path in annotation_paths:
            assert annotation_path.endswith(
                ".jsonl"
            ), "Please organize annotation data as JSONL (one sample per line) and store the final line as the seek index."
            root, fname = os.path.split(annotation_path)
            stem = fname.replace("_with_seeks", "").rsplit(".jsonl", 1)[0]
            seek_path = os.path.join(root, f"{stem}_seeks.jsonl")

            logger.warning(f"Loading {annotation_path}")
            logger.warning(f"Loading seek index from {seek_path}")

            with open(seek_path) as f:
                seeks = json.load(f)

            self.handles.extend(zip([annotation_path] * len(seeks), seeks))
            logger.warning(f"Successfully loaded {annotation_path}")

        if "Qwen2VL" in processor.__class__.__name__:
            self.im_start_id, self.assistant_id, self.newline_id, self.im_end_id = (
                processor.tokenizer("<|im_start|>assistant\n<|im_end|>").input_ids
            )
            self.get_range = get_qwen_range
            self.model_base = "Qwen2"
        elif "Qwen2_5_VL" in processor.__class__.__name__:
            self.im_start_id, self.assistant_id, self.newline_id, self.im_end_id = (
                processor.tokenizer("<|im_start|>assistant\n<|im_end|>").input_ids
            )
            self.get_range = get_qwen_range
            self.model_base = "Qwen2"
        else:
            raise NotImplementedError(
                f"Video preprocessing for {processor.__class__.__name__} is not implemented"
            )

        self.processor = processor
        self.with_context = with_context
        self.initial_fps_frames = initial_fps_frames
        self.streaming_fps_frames = streaming_fps_frames
        self.text_sink = text_sink
        self.text_sliding_window = text_sliding_window

    def load_conversation(self, index):
        """Load a single conversation by index."""
        annotation_path, seek = self.handles[index]
        with open(annotation_path) as f:
            f.seek(seek)
            line = f.readline()
        line = json.loads(line)
        return line

    def preprocess_image(self, element: dict):
        """Preprocess image data."""
        if hasattr(self, "remote_loader"):
            return Image.open(self.remote_loader(element["image"]))
        return element["image"]

    def preprocess_video(self, element: dict):
        """Preprocess video data."""
        if "pos" in element:
            positions = [0] + element["pos"]
            nframes = smart_nframes(
                element, total_frames=len(positions) - 1, video_fps=FPS
            )
            sampler = torch.linspace(0, len(positions) - 2, nframes).round().long()
            data_bytes = self.remote_loader(
                element["video"], length_check=True, return_io=False
            )
            video = torch.stack(
                [
                    pil_to_tensor(
                        bytes_to_pil(data_bytes[positions[i] : positions[i + 1]])
                    )
                    for i in sampler
                ]
            )
            video = _spatial_resize_video(video)
            return video
        return element["video"]

    def preprocess_text(self, element: str):
        """Preprocess text data."""
        return element["text"]

    def preprocess_conversation_stream(self, conversation: list):
        """Simulate converting timestamped conversation into a streaming multi-turn dialogue."""
        user_message, assistant_message = conversation
        user_content, assistant_content = (
            user_message["content"],
            assistant_message["content"],
        )

        user_video_dict, user_query_dict = user_content
        video_start = user_video_dict["video_start"]
        video_end = user_video_dict["video_end"]

        assert (
            "video" in user_video_dict
        ), "Please check your data: the first user content must contain video information"

        assistant_text_stream = assistant_message["content"][0]["text_stream"]
        qa_stream = (
            assistant_message["content"][0]["qa_stream"]
            if "qa_stream" in assistant_message["content"][0]
            else []
        )

        with mute_stderr_ffmpeg():
            clip, _, clip_pts = _read_video_decord_plus(
                user_video_dict, return_pts=True, strict_fps=True
            )
        clip = _spatial_resize_video(clip)

        start_timestamp, end_timestamp = (
            video_start,
            video_start + self.initial_fps_frames / FPS,
        )

        phrase, next_start_from = get_phrase_before_timestamp(
            assistant_text_stream, end_timestamp
        )

        if (
            len(qa_stream) > 0
            and start_timestamp < qa_stream[0][1]
            and end_timestamp >= qa_stream[0][1]
        ):
            question = qa_stream[0][2]
            answer = qa_stream[0][3]
            qa_stream = qa_stream[1:]
        else:
            question = ""
            answer = ""

        user_content = [
            {
                "type": "text",
                "text": f"Time={start_timestamp:.1f}-{end_timestamp:.1f}s"
                + f"{question}",
            },
            {"type": "video", "video": clip[: self.initial_fps_frames]},
        ]
        assistant_content = [{"type": "text", "text": answer + "\n" + phrase + " ..."}]
        conversation = [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": assistant_content},
        ]
        frames_list = [clip[: self.initial_fps_frames]]

        for i in range(self.initial_fps_frames, len(clip), self.streaming_fps_frames):
            start_timestamp, end_timestamp = (
                video_start + i / FPS,
                video_start + (i + self.streaming_fps_frames) / FPS,
            )

            phrase, next_start_from = get_phrase_before_timestamp(
                assistant_text_stream, end_timestamp, start_from=next_start_from
            )
            if (
                len(qa_stream) > 0
                and start_timestamp < qa_stream[0][1]
                and end_timestamp >= qa_stream[0][1]
            ):
                question = qa_stream[0][2]
                answer = qa_stream[0][3]
                qa_stream = qa_stream[1:]
            else:
                question = ""
                answer = ""

            frames = clip[i : i + self.streaming_fps_frames]

            user_content = [
                {
                    "type": "text",
                    "text": f"Time={start_timestamp:.1f}-{end_timestamp:.1f}s {question}",
                },
                {"type": "video", "video": frames},
            ]
            assistant_content = [
                {"type": "text", "text": answer + "\n" + phrase + " ..."}
            ]

            conversation.extend(
                [
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": assistant_content},
                ]
            )
            frames_list.append(frames)

        return conversation, frames_list

    def getitem(self, index, return_text=False):
        """Core logic to get and preprocess a single data item."""
        conversation = self.load_conversation(index)

        special_process_for_stream, image_inputs, video_inputs = False, None, None
        previous_text = ""
        for message in conversation:
            if message["role"] == "user":
                for element in message["content"]:
                    if "previous" in element:
                        previous_text = element["previous"]
                        element["previous"] = ""
                    if hasattr(self, "remote_loader"):
                        element["remote_loader"] = self.remote_loader
                    modal = element["type"]
                    element[modal] = getattr(self, f"preprocess_{modal}")(element)
                    if isinstance(element[modal], torch.Tensor):
                        if video_inputs is None:
                            video_inputs = [element[modal]]
                        else:
                            video_inputs.append(element[modal])
            else:
                for element in message["content"]:
                    special_process_for_stream = "text_stream" in element
                    break

        if not os.path.exists(conversation[0]["content"][0]["video"]):
            if os.path.exists(
                os.path.join(
                    os.environ["DATASET_PATH"], conversation[0]["content"][0]["video"]
                )
            ):
                conversation[0]["content"][0]["video"] = os.path.join(
                    os.environ["DATASET_PATH"], conversation[0]["content"][0]["video"]
                )
            else:
                raise ValueError(
                    f"Video {conversation[0]['content'][0]['video']} not found"
                )
        if special_process_for_stream:
            conversation, video_inputs = self.preprocess_conversation_stream(
                conversation
            )
            image_inputs = None
        else:
            if not video_inputs and not image_inputs:
                image_inputs, video_inputs = process_vision_info(conversation)

        conversation = [
            {"role": "previous text", "content": previous_text}
        ] + conversation

        if return_text:
            return conversation
        texts = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=False,
            return_tensors="pt",
        )

        inputs = self.processor(
            text=texts,
            images=image_inputs,
            videos=video_inputs,
            return_tensors="pt",
        )

        if self.text_sink != 0 or self.text_sliding_window != 0:
            previous_text_start_idx, previous_text_end_idx = self.get_range(
                inputs.input_ids, "previous text", 0, contain_lf=True
            )
            need_truncate = (
                previous_text_start_idx + self.text_sink + self.text_sliding_window
                <= previous_text_end_idx + 1
            )
            if need_truncate:
                truncate_start = previous_text_start_idx + self.text_sink
                truncate_end = previous_text_end_idx - self.text_sliding_window
                inputs["input_ids"] = torch.cat(
                    [
                        inputs.input_ids[:, :truncate_start],
                        inputs.input_ids[:, truncate_end + 1 :],
                    ],
                    dim=1,
                ).contiguous()
                if "attention_mask" in inputs:
                    inputs["attention_mask"] = torch.cat(
                        [
                            inputs.attention_mask[:, :truncate_start],
                            inputs.attention_mask[:, truncate_end + 1 :],
                        ],
                        dim=1,
                    ).contiguous()

        input_ids = inputs.input_ids
        labels = torch.full_like(input_ids, fill_value=-100, dtype=input_ids.dtype)
        im_start_idxs = (input_ids == self.im_start_id).nonzero()
        im_end_idxs = (input_ids == self.im_end_id).nonzero()
        for (sample_idx, im_start_idx), (sample_idx, im_end_idx) in zip(
            im_start_idxs, im_end_idxs
        ):
            if input_ids[sample_idx, im_start_idx + 1] == self.assistant_id:
                labels[sample_idx, im_start_idx + 3 : im_end_idx + 1] = input_ids[
                    sample_idx, im_start_idx + 3 : im_end_idx + 1
                ]

        inputs["labels"] = labels
        if self.return_conversation:
            inputs["conversation"] = conversation
            inputs["start_timestamp"] = conversation[0]["content"][0]["video_start"]

        return inputs

    def __getitem__(self, index):
        """Dataset standard method with retry."""
        try:
            return self.getitem(index)
        except Exception as e:
            logger.warning(
                f"{'Training' if not self.is_eval_dataset else 'Eval'}: bug at video: "
                f"{self.load_conversation(index)[0]['content'][0]['video']}"
                f"{e}"
            )
        return self.__getitem__(index * 13 % len(self.handles))

    def data_collator(self, batched_inputs, **kwargs):
        assert len(batched_inputs) == 1, "batch size must be 1"
        return batched_inputs[0]

    def __len__(self):
        """Return the total number of samples."""
        return len(self.handles)


def get_ground_truth(dataset, idx, processor):
    video_text = dataset.getitem(idx, return_text=True)
    ground_truths = []
    for round in video_text:
        if round["role"] == "assistant":
            ground_truth = round["content"][0]["text"]
            ground_truths.append({"ground_truth": ground_truth})
        else:
            continue
    return ground_truths


if __name__ == "__main__":
    import argparse

    set_seed(1314)
    args = argparse.ArgumentParser()
    args.add_argument("--data_path", type=List[str], default=[])
    args.add_argument("--model_base", type=str, default="Qwen", choices=["Qwen"])
    args.add_argument("--idx", type=int, default=None)
    args.add_argument("--text_sink", type=int, default=512)
    args.add_argument("--text_sliding_window", type=int, default=512)
    args = args.parse_args()

    from transformers import AutoProcessor

    processor = AutoProcessor.from_pretrained(
        "Qwen/Qwen2-VL-7B-Instruct", padding_side="right"
    )
    if args.idx is not None:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True,
            device_map="auto",
            attn_implementation="flash_attention_2",
        )

    dataset = LMMDataset(
        train_annotation_paths=args.data_path,
        tokenizer=None,
        processor=processor,
        text_sink=args.text_sink,
        text_sliding_window=args.text_sliding_window,
        with_context=False,
    )
    from torch.utils.data import DataLoader

    dataloader = DataLoader(
        dataset,
        batch_size=1,
        shuffle=True,
        num_workers=128,
        collate_fn=dataset.data_collator,
    )

    if args.idx is not None:
        data = dataset[args.idx]
        breakpoint()

        model.generate(
            input_ids=data.input_ids.to("cuda"),
            media=data.media,
            media_config=defaultdict(dict),
            generation_config=model.default_generation_config,
        )
        breakpoint()
    else:
        for i, batch in tqdm.tqdm(enumerate(dataloader)):
            pass
